{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook for Training Models and Generating Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some flags which will affect the way the notebook executes and what data is written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs notebook in a mode which uses max-normalized spectra \n",
    "#(figures for this can be found in the paper's supplemental information.)\n",
    "# Maintext figures set this variable to FALSE.\n",
    "use_max_normalized = False\n",
    "norm_str = 'max' if use_max_normalized else 'feff'\n",
    "\n",
    "# Flag for using validation data (for model characterization)\n",
    "# or testing data (should only be done once the previous process is complete).\n",
    "# Default to testing data.\n",
    "use_test = True\n",
    "\n",
    "# Set random seed to be used as argument for other functions.\n",
    "rseed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T20:31:41.671082Z",
     "start_time": "2019-10-18T20:31:33.060018Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install pymatgen\n",
    "# !{sys.executable} -m pip install trixs\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "#import eli5 # not needed\n",
    "#from eli5.sklearn import PermutationImportance # not needed\n",
    "from keras.regularizers import l2 # Regularization in Keras L2: Sum of the squared weights.\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from numpy import zeros, newaxis\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T20:31:41.671082Z",
     "start_time": "2019-10-18T20:31:33.060018Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import sklearn\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.stats import norm\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pprint import pprint\n",
    "\n",
    "from pymatgen.core import Structure\n",
    "from pymatgen.analysis.structure_matcher import StructureMatcher, ElementComparator\n",
    "\n",
    "#from trixs.machine_learning.benchmarks import precision_recall_matrix, confusion_dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from trixs.spectra.spectrum_featurize import polynomialize_by_idx, gauge_polynomial_error\n",
    "\n",
    "storage_directory = './spectral_data'\n",
    "\n",
    "figure_write_folder = \"./figures_feffnorm\" if not use_max_normalized else './figures_maxnorm'\n",
    "try: \n",
    "    os.mkdir(figure_write_folder) \n",
    "except OSError as error: \n",
    "    pass\n",
    "np.random.seed(rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The publication uses SKlearn version 0.21.3. Yours: 0.23.1\n"
     ]
    }
   ],
   "source": [
    "print(\"The publication uses SKlearn version 0.21.3. Yours:\",sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define domains which will be used for x-axis labels later, as well as define the elements which will be imported for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T20:31:41.687034Z",
     "start_time": "2019-10-18T20:31:41.673251Z"
    }
   },
   "outputs": [],
   "source": [
    "target_elements_groups=[('Ti','O'),('V','O'),('Cr','O'),\n",
    "                        ('Mn','O'),('Fe','O'),('Co','O'),\n",
    "                        ('Ni','O'),('Cu','O')]\n",
    "\n",
    "x_domains = {  ('Co','O'):  np.linspace(7713.5, 7765.83,100),\n",
    "               ('Fe','O'): np.linspace(7115.0, 7167.764,100),\n",
    "               ('V','O'):  np.linspace(5468.0, 5520.631,100),\n",
    "               ('Cu','O'): np.linspace( 8987.5, 9039.712,100),\n",
    "               ('Ni','O'): np.linspace( 8336.5 ,8388.723,100),\n",
    "               ('Cr','O'): np.linspace(5993.1, 6045.686,100),\n",
    "               ('Mn','O'): np.linspace(6541.7, 6594.417,100),\n",
    "               ('Ti','O'): np.linspace(4969.0, 5021.024,100)}\n",
    "\n",
    "colors_by_pair = {('Ti','O'):'orangered',\n",
    "                  ('V','O'):'darkorange',\n",
    "                  ('Cr','O'):'gold',\n",
    "                  ('Mn','O'):'seagreen',\n",
    "                  ('Fe','O'):'dodgerblue',\n",
    "                  ('Co','O'):'navy',\n",
    "                  ('Ni','O'):'rebeccapurple',\n",
    "                  ('Cu','O'):\"mediumvioletred\"}\n",
    "\n",
    "pair_to_name={'Ti':\"Titanium\",'V':'Vanadium',\n",
    "              'Cr':'Chromium','Mn':\"Manganese\",\n",
    "              'Fe':\"Iron\",'Co':\"Cobalt\",\n",
    "             'Ni':'Nickel','Cu':'Copper'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Pointwise Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Precision / Recall Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T20:31:51.246117Z",
     "start_time": "2019-10-18T20:31:51.213273Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "\n",
    "def precision_recall(fits: List, labels: List, target)->List[float]:\n",
    "    \"\"\"\n",
    "    Computes the precision and recall and F1 score\n",
    "    for an individual class label 'target',\n",
    "    which can be any object with an equivalence relation via ==\n",
    "    :param fits:\n",
    "    :param labels:\n",
    "    :param target:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    N = len(labels)\n",
    "\n",
    "    # Generate the counts of true and false positives\n",
    "    true_positives = len([True for i in range(N)\n",
    "                          if (fits[i] == target and labels[i] == target)])\n",
    "    false_positives = len([True for i in range(N)\n",
    "                           if (fits[i] == target and labels[i] != target)])\n",
    "    false_negatives = len([True for i in range(N)\n",
    "                           if (fits[i] != target and labels[i] == target)])\n",
    "\n",
    "    if true_positives == 0:\n",
    "        return [0, 0, 0]\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1 = 2.0 * precision * recall / (precision + recall)\n",
    "    return [precision, recall, f1]\n",
    "\n",
    "\n",
    "def precision_recall_matrix(fits: List, labels: List, classes: List):\n",
    "    \"\"\"\n",
    "    Computes the precision and recall and F1 score for a set of classes at once\n",
    "\n",
    "    :param fits:\n",
    "    :param classes:\n",
    "    :param labels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for cls in classes:\n",
    "        results.append(precision_recall(fits, labels, cls))\n",
    "    return np.array(results)\n",
    "\n",
    "def avg_f1_score(guesses,labels):\n",
    "    f1_score = precision_recall_matrix(guesses,labels,[4,5,6])\n",
    "    return np.mean([np.round(100*x[2],1) for x in f1_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Train/Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TT stands for Train-Test\n",
    "# c = coord, b = bader, md = mean distance\n",
    "ttc_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "ttb_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "ttmd_by_pair = {pair:{} for pair in target_elements_groups}\n",
    "\n",
    "for pair in target_elements_groups:\n",
    "    for key in ['train_x','train_y','valid_x','valid_y','test_x','test_y']:\n",
    "        ttc_by_pair[pair][key] =np.load(f'./model_data/{pair[0]}_coord_{key}.npy')\n",
    "        ttb_by_pair[pair][key] =np.load(f'./model_data/{pair[0]}_bader_{key}.npy')\n",
    "        ttmd_by_pair[pair][key] =np.load(f'./model_data/{pair[0]}_md_{key}.npy')\n",
    "        \n",
    "# Quickly normalize the input X spectra if toggled at top of notebook.\n",
    "if use_max_normalized:\n",
    "    for pair in target_elements_groups:\n",
    "        for key in ['train_x','valid_x','test_x']:\n",
    "            ttc_by_pair[pair][key] = np.array([array / np.max(array) for array in ttc_by_pair[pair][key][:]])\n",
    "            ttb_by_pair[pair][key] = np.array([array / np.max(array) for array in ttb_by_pair[pair][key][:]])\n",
    "            ttmd_by_pair[pair][key] = np.array([array / np.max(array) for array in ttmd_by_pair[pair][key][:]])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Cell 1:\n",
    "# Random Forests Trained using Pointwise spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T21:56:33.024033Z",
     "start_time": "2019-10-18T20:31:33.099Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing run...\n"
     ]
    }
   ],
   "source": [
    "# Flag to run or not run the cell\n",
    "run = True\n",
    "# Flag to display plots inline\n",
    "show_plots = False\n",
    "print(\"Commencing run...\")\n",
    "\n",
    "accuracies = {}\n",
    "deviations = {}\n",
    "all_data_values = []\n",
    "\n",
    "accuracies_nn = {}\n",
    "deviations_nn = {}\n",
    "all_data_values_nn = []\n",
    "\n",
    "md_perf_by_pair={}\n",
    "bader_perf_by_pair={}\n",
    "models_by_pair = {}\n",
    "means_by_pair = {}\n",
    "\n",
    "md_perf_by_pair_nn={}\n",
    "bader_perf_by_pair_nn={}\n",
    "models_by_pair_nn = {}\n",
    "means_by_pair_nn = {}\n",
    "\n",
    "\n",
    "use_test = True\n",
    "\n",
    "# Flags for your own experimentaion purposes if you'd like to focus on one task or another.\n",
    "# N O T E ! The rest of the notebook assumes all three flags are on!\n",
    "run_coord = True\n",
    "run_bader = True\n",
    "run_md = True\n",
    "\n",
    "# How many times to repeat the training of the random forests \n",
    "# with different random seeds used\n",
    "# for the training, to generate error bars \n",
    "# on the feature RANKing (hence, RANK REPEAT).\n",
    "\n",
    "# The publication uses 10 total trainings, so RANK_REPEAT is set here to 9.\n",
    "RANK_REPEAT = 9\n",
    "\n",
    "\n",
    "# The hyperparameter N_ESTIMATORS is controllable here because running the notebook with\n",
    "# a smaller number of estimators may be desirable to verify that things are working.\n",
    "# Set to 300 by default-- as was used in the publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T21:56:33.024033Z",
     "start_time": "2019-10-18T20:31:33.099Z"
    },
    "code_folding": []
   },
   "source": [
    "## Define nerual network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T21:56:33.024033Z",
     "start_time": "2019-10-18T20:31:33.099Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# define baseline model for coord w/o regularization term L2\n",
    "def baseline_model_c():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=100, activation='relu')) # the input X dim is 100\n",
    "    model.add(Dense(100, activation='relu')) # more hidden layer\n",
    "    model.add(Dense(50, activation='relu')) # more hidden layer\n",
    "    model.add(Dense(3, activation='softmax')) # the outpu Y dim is 3\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define regression base_model for b w/o regularization term L2\n",
    "def base_model_b():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=100, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(100, activation='relu')) # more hidden layer\n",
    "    model.add(Dense(50, activation='relu')) # more hidden layer\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# define regression base_model for md w/o regularization term L2\n",
    "def base_model_md():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(150, input_dim=100, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(100, kernel_initializer='normal', activation='relu')) # more hidden layer\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation='relu')) # more hidden layer\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T21:56:33.024033Z",
     "start_time": "2019-10-18T20:31:33.099Z"
    },
    "code_folding": []
   },
   "source": [
    "## Define CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T21:56:33.024033Z",
     "start_time": "2019-10-18T20:31:33.099Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def yconvert(y):\n",
    "    if y == 4:\n",
    "        return([1,0,0])\n",
    "    if y == 5:\n",
    "        return([0,1,0])\n",
    "    if y == 6:\n",
    "        return([0,0,1])\n",
    "\n",
    "def ycnn(yc_train):\n",
    "    return np.array([yconvert(y) for y in yc_train])\n",
    "\n",
    "def xcnn(xc_train):\n",
    "    return(xc_train[:, :, newaxis])\n",
    "\n",
    "\n",
    "# cnn model for corrdinate\n",
    "def cnn_model_c():\n",
    "    n_features, n_timesteps, n_outputs = 100, 1, 3\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=10, activation='relu', input_shape=(n_features, n_timesteps)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=10, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#nn_c = KerasClassifier(build_fn=cnn_model_c, epochs=10, batch_size=32, verbose=1)\n",
    "#nn_c.fit(xcnn(xc_train), ycnn(yc_train))\n",
    "#print('training accuracy: ', nn_c.score(xcnn(xc_train), ycnn(yc_train)))\n",
    "#print('testing accuracy: ', nn_c.score(xcnn(xc_valid), ycnn(yc_valid)))\n",
    "\n",
    "\n",
    "# cnn model for bader\n",
    "def cnn_model_b():\n",
    "    n_features, n_timesteps, n_outputs = 100, 1, 1\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=10, kernel_initializer='normal', activation='relu', input_shape=(n_features, n_timesteps)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(n_outputs, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "#nn_b = KerasRegressor(build_fn=cnn_model_b, epochs=100, batch_size=32, verbose=1)\n",
    "#nn_b.fit(xcnn(xb_train), yb_train)\n",
    "#print(\"R2:\", r2_score(yb_valid, nn_b.predict(xcnn(xb_valid)))) # R-Squared\n",
    "#print(\"MSE:\", np.mean(np.abs(nn_b.predict(xcnn(xb_valid)) - yb_valid)))\n",
    "\n",
    "# cnn model for md\n",
    "def cnn_model_md():\n",
    "    n_features, n_timesteps, n_outputs = 100, 1, 1\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=10, kernel_initializer='normal', activation='relu', input_shape=(n_features, n_timesteps)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=10, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(AveragePooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(200, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(n_outputs, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "#nn_md = KerasRegressor(build_fn=cnn_model_md, epochs=10, batch_size=N_EPOCHS, verbose=1)\n",
    "#nn_md.fit(xcnn(xmd_train), ymd_train)\n",
    "#print(\"R2:\", r2_score(ymd_valid, nn_md.predict(xcnn(xmd_valid)))) # R-Squared\n",
    "#print(\"MSE:\", np.mean(np.abs(nn_md.predict(xcnn(xmd_valid)) - ymd_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T21:56:33.024033Z",
     "start_time": "2019-10-18T20:31:33.099Z"
    },
    "code_folding": []
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T21:56:33.024033Z",
     "start_time": "2019-10-18T20:31:33.099Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Data Augmentation: x +/- 0.5;\n",
    "def x_augmentation(x, delta=0.5):\n",
    "    x0 = list(x)\n",
    "    x1 = x + delta\n",
    "    x2 = x - delta\n",
    "    x0.extend(list(x1))\n",
    "    x0.extend(list(x2))\n",
    "    return(np.array(x0))\n",
    "\n",
    "## Data Augmentation: randomly add any value between -0.03 and +0.03 \n",
    "def y_augmentation(y, delta=0.03):\n",
    "    y0 = list(y)\n",
    "    noise1 = np.random.normal(0,1,len(y)) \n",
    "    noise2 = np.random.normal(0,1,len(y)) \n",
    "    #y1 = y + y*delta*noise1\n",
    "    #y2 = y + y*delta*noise2\n",
    "    y1 = y + delta*noise1\n",
    "    y2 = y + delta*noise2\n",
    "    y0.extend(y1)\n",
    "    y0.extend(y2)\n",
    "    return(np.array(y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T21:56:33.024033Z",
     "start_time": "2019-10-18T20:31:33.099Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "target_elements_groups:   0%|                                       | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation coord:   0%|                                       | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Cross Validation coord:  50%|████████████▌            | 1/2 [1:16:33<1:16:33, 4593.79s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "#N_ESTIMATORS = 300\n",
    "#N_EPOCHS = 150\n",
    "# RANK_REPEAT = 9\n",
    "\n",
    "N_ESTIMATORS = 300\n",
    "N_EPOCHS = 300\n",
    "RANK_REPEAT = 2\n",
    "VERBOSE = 0\n",
    "pair = target_elements_groups[0]\n",
    "\n",
    "pair = target_elements_groups[0]\n",
    "for pair in tqdm(target_elements_groups, ncols=90, desc='target_elements_groups'):\n",
    "    if not run:\n",
    "        continue\n",
    "    \n",
    "    # Instantiate each random forest model\n",
    "    \n",
    "    forest_c = RandomForestClassifier(random_state=rseed,\n",
    "                                      n_estimators=N_ESTIMATORS,\n",
    "                                      max_depth =35, \n",
    "                                      max_features = 8, min_samples_leaf = 1,\n",
    "                                      min_samples_split = 2,\n",
    "                                      class_weight=None,\n",
    "                                      n_jobs=4)\n",
    "\n",
    "\n",
    "    forest_b = RandomForestRegressor(random_state=rseed,\n",
    "                                     criterion='mse',max_depth=35,\n",
    "                                     max_features=8, max_leaf_nodes=None,\n",
    "                                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                     min_samples_leaf=1, min_samples_split=2,\n",
    "                                     min_weight_fraction_leaf=0.0, n_estimators=N_ESTIMATORS,\n",
    "                                     n_jobs=4)\n",
    "    \n",
    "    forest_md = RandomForestRegressor(random_state=rseed,\n",
    "                                      criterion='mse',max_depth=35,\n",
    "                                      max_features=8, max_leaf_nodes=None,\n",
    "                                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                      min_samples_leaf=1, min_samples_split=2,\n",
    "                                      min_weight_fraction_leaf=0.0, n_estimators=N_ESTIMATORS,\n",
    "                                      n_jobs=4)\n",
    "\n",
    "    # Instantiate each neural nets model\n",
    "    # nn_c = KerasClassifier(build_fn=baseline_model_c, epochs=N_EPOCHS, batch_size=32, verbose=VERBOSE)\n",
    "    \n",
    "    # nn_b = Pipeline([('standardize', StandardScaler()),\n",
    "    #                  ('estimator', KerasRegressor(build_fn=base_model_b, epochs=N_EPOCHS, batch_size=32, verbose=VERBOSE))])\n",
    "    # nn_md = Pipeline([('standardize', StandardScaler()),\n",
    "    #                   ('estimator', KerasRegressor(build_fn=base_model_md, epochs=N_EPOCHS, batch_size=32, verbose=VERBOSE))])\n",
    "\n",
    "    nn_c = KerasClassifier(build_fn=cnn_model_c, epochs=N_EPOCHS, batch_size=32, verbose=VERBOSE)\n",
    "    nn_b = KerasRegressor(build_fn=cnn_model_b, epochs=N_EPOCHS, batch_size=32, verbose=VERBOSE)\n",
    "    nn_md = KerasRegressor(build_fn=cnn_model_md, epochs=N_EPOCHS, batch_size=32, verbose=VERBOSE)\n",
    "\n",
    "    #############################################\n",
    "    # COORDINATION\n",
    "    #############################################\n",
    "    \n",
    "    xc_train = ttc_by_pair[pair]['train_x'] \n",
    "    yc_train = ttc_by_pair[pair]['train_y'] \n",
    "    xc_train = x_augmentation(xc_train, delta=0.5)\n",
    "    yc_train = y_augmentation(yc_train, delta=0.00)\n",
    "\n",
    "    if use_test:\n",
    "        xc_valid = ttc_by_pair[pair]['test_x']\n",
    "        yc_valid = ttc_by_pair[pair]['test_y'] \n",
    "    else:\n",
    "        xc_valid = ttc_by_pair[pair]['valid_x'] \n",
    "        yc_valid = ttc_by_pair[pair]['valid_y']\n",
    "        xc_valid = x_augmentation(xc_valid, delta=0.5)\n",
    "        yc_valid = y_augmentation(yc_valid, delta=0.00)\n",
    "            \n",
    "    \n",
    "    if run_coord:\n",
    "        forest_c.fit(xc_train,yc_train)        \n",
    "        nn_c.fit(xcnn(xc_train),ycnn(yc_train))\n",
    "\n",
    "        ################## FEATURE RANKING VARIANCE INTERLUDE ######################\n",
    "\n",
    "        cur_model_f1s = [[x[2]*100 for x in  precision_recall_matrix(forest_c.predict(xc_valid),yc_valid,[4,5,6])]]\n",
    "        cur_model_accuracies = [forest_c.score(xc_valid,yc_valid)]\n",
    "        cur_model_importances = [forest_c.feature_importances_] \n",
    "\n",
    "        nn_model_f1s = [[x[2]*100 for x in  precision_recall_matrix(nn_c.predict(xcnn(xc_valid))+4,yc_valid,[4,5,6])]]\n",
    "        nn_model_accuracies = [nn_c.score(xcnn(xc_valid), ycnn(yc_valid))]\n",
    "        \n",
    "        for i in tqdm(range(RANK_REPEAT), ncols=90, desc='Cross Validation coord'):\n",
    "            forest_c.random_state = rseed+i+1\n",
    "            forest_c.fit(xc_train,yc_train)\n",
    "            cur_model_importances.append(forest_c.feature_importances_)\n",
    "            cur_model_accuracies.append(forest_c.score(xc_valid,yc_valid))\n",
    "            cur_model_f1s.append([x[2]*100 for x in precision_recall_matrix(forest_c.predict(xc_valid),yc_valid,[4,5,6])])\n",
    "\n",
    "            nn_c.fit(xcnn(xc_train),ycnn(yc_train))\n",
    "            nn_model_accuracies.append(nn_c.score(xcnn(xc_valid),ycnn(yc_valid)))\n",
    "            nn_model_f1s.append([x[2]*100 for x in precision_recall_matrix(nn_c.predict(xcnn(xc_valid))+4,yc_valid,[4,5,6])])\n",
    "\n",
    "        importances_mean = np.mean(cur_model_importances,axis=0)\n",
    "        coord_accuracies_mean = np.mean(cur_model_accuracies)\n",
    "        coord_f1s_mean = np.mean(cur_model_f1s, axis=0)\n",
    "\n",
    "        nn_coord_accuracies_mean = np.mean(nn_model_accuracies)\n",
    "        nn_coord_f1s_mean = np.mean(nn_model_f1s, axis=0)\n",
    "\n",
    "        if RANK_REPEAT:\n",
    "            importances_std = np.std(cur_model_importances,axis=0)\n",
    "            coord_accuracies_std = np.std(cur_model_accuracies)\n",
    "            coord_f1s_std = np.std(cur_model_f1s,   axis=0)\n",
    "            nn_coord_accuracies_std = np.std(nn_model_accuracies)\n",
    "            nn_coord_f1s_std = np.std(nn_model_f1s,   axis=0)\n",
    "        else:\n",
    "            importances_std = np.zeros(len(cur_model_importances))\n",
    "            coord_accuracies_std = np.zeros(len(cur_model_accuracies))\n",
    "            coord_f1s_std = np.zeros(len(cur_model_f1s))\n",
    "            nn_coord_accuracies_std = np.zeros(len(nn_model_accuracies))\n",
    "            nn_coord_f1s_std = np.zeros(len(nn_model_f1s))\n",
    "\n",
    "\n",
    "        means_by_pair[str(pair)+'-coord'] = importances_mean\n",
    "\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.errorbar(x_domains[pair],importances_mean,yerr= importances_std,\n",
    "                     label='RF: $\\mu$ $\\pm$ 1 $\\sigma$ (N={})'.format(1+RANK_REPEAT),\n",
    "                     color='black',ecolor='blue')\n",
    "        plt.title(\"Model Importance Spread \\n{} Coordination (Pointwise)\".format(pair[0]))\n",
    "        plt.legend()\n",
    "        plt.savefig(figure_write_folder+'/{}_{}_all_coord_mean_importances.pdf'.format(pair[0],norm_str),\n",
    "                    format='pdf',dpi=300,transparent=True,bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Store the last model trained for later \n",
    "        # use in comparing against polynomial models.\n",
    "        models_by_pair[str(pair)+'-Coord'] = forest_c\n",
    "        models_by_pair_nn[str(pair)+'-Coord'] = nn_c\n",
    "\n",
    "        print(\"Done with Coordination for \",pair)\n",
    "        \n",
    "        class_makeup = Counter(yc_valid)\n",
    "        mode_guess_score = max(class_makeup.values())/sum(class_makeup.values())\n",
    "    #############################################\n",
    "    # BADER\n",
    "    #############################################\n",
    "    \n",
    "    xb_train =  ttb_by_pair[pair]['train_x']\n",
    "    yb_train =  ttb_by_pair[pair]['train_y']\n",
    "    xb_train = x_augmentation(xb_train, delta=0.5)\n",
    "    yb_train = y_augmentation(yb_train, delta=0.03)\n",
    "    \n",
    "    if use_test:\n",
    "        xb_valid = ttb_by_pair[pair]['test_x']\n",
    "        yb_valid = ttb_by_pair[pair]['test_y']\n",
    "    else:\n",
    "        xb_valid = ttb_by_pair[pair]['valid_x']\n",
    "        yb_valid = ttb_by_pair[pair]['valid_y']\n",
    "        xb_valid = x_augmentation(xb_valid, delta=0.5)\n",
    "        yb_valid = y_augmentation(yb_valid, delta=0.03)\n",
    " \n",
    "    \n",
    "    if run_bader:\n",
    "        forest_b.fit(xb_train,yb_train)    \n",
    "        nn_b.fit(xcnn(xb_train), yb_train)\n",
    "\n",
    "        cur_model_importances = [forest_b.feature_importances_] \n",
    "        cur_model_accuracies = [r2_score(yb_valid,forest_b.predict(xb_valid))] # R-Squared\n",
    "        cur_model_maes = [np.mean(np.abs(forest_b.predict(xb_valid) - yb_valid))]\n",
    "\n",
    "        nn_model_accuracies = [r2_score(yb_valid,nn_b.predict(xcnn(xb_valid)))] # R-Squared\n",
    "        nn_model_maes = [np.mean(np.abs(nn_b.predict(xcnn(xb_valid)) - yb_valid))]\n",
    "\n",
    "        for i in tqdm(range(RANK_REPEAT), ncols=90, desc='Cross Validation BADER'):\n",
    "            forest_b.random_state = rseed+i+1\n",
    "            forest_b.fit(xb_train,yb_train)\n",
    "            cur_model_importances.append(forest_b.feature_importances_)\n",
    "            cur_model_accuracies.append(r2_score(forest_b.predict(xb_valid), yb_valid))\n",
    "            cur_model_maes.append(np.mean(np.abs(forest_b.predict(xb_valid) - yb_valid)))\n",
    "\n",
    "            nn_b.fit(xcnn(xb_train),yb_train)\n",
    "            nn_model_accuracies.append(r2_score(nn_b.predict(xcnn(xb_valid)),yb_valid))\n",
    "            nn_model_maes.append(np.mean(np.abs(nn_b.predict(xcnn(xb_valid)) - yb_valid)))\n",
    "\n",
    "        importances_mean = np.mean(cur_model_importances,axis=0)\n",
    "        bader_accuracies_mean  = np.mean(cur_model_accuracies)\n",
    "        bader_maes_mean = np.mean(cur_model_maes)\n",
    "\n",
    "        nn_bader_accuracies_mean  = np.mean(nn_model_accuracies)\n",
    "        nn_bader_maes_mean = np.mean(nn_model_maes)\n",
    "\n",
    "        if RANK_REPEAT:\n",
    "            importances_std  = np.std(cur_model_importances,axis=0)\n",
    "            bader_accuracies_std   = np.std(cur_model_accuracies)\n",
    "            bader_maes_std = np.std(cur_model_maes)\n",
    "            nn_bader_accuracies_std   = np.std(nn_model_accuracies)\n",
    "            nn_bader_maes_std = np.std(nn_model_maes)\n",
    "        else:\n",
    "            importances_std = np.zeros(len(cur_model_importances))\n",
    "            bader_accuracies_std = np.zeros(len(cur_model_accuracies))\n",
    "            bader_maes_std = np.zeros(len(cur_model_maes))\n",
    "            nn_bader_accuracies_std = np.zeros(len(nn_model_accuracies))\n",
    "            nn_bader_maes_std = np.zeros(len(nn_model_maes))\n",
    "\n",
    "\n",
    "        accuracies[str(pair)+'-Bader'] = bader_accuracies_mean\n",
    "        deviations[str(pair)+'-Bader'] = bader_accuracies_std\n",
    "        means_by_pair[str(pair)+'-Bader'] = importances_mean\n",
    "\n",
    "        accuracies_nn[str(pair)+'-Bader'] = nn_bader_accuracies_mean\n",
    "        deviations_nn[str(pair)+'-Bader'] = nn_bader_accuracies_std\n",
    "\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.errorbar(x_domains[pair],importances_mean, yerr=importances_std,\n",
    "                     label='RF: $\\mu$ $\\pm$ 1 $\\sigma$ (N={})'.format(1+RANK_REPEAT),\n",
    "                     color='black',ecolor='red')\n",
    "        plt.title(\"Model Importance Spread \\nFor {} Bader Charge (Pointwise)\".format(pair[0]))\n",
    "        plt.legend()\n",
    "        plt.savefig(figure_write_folder+'/{}_{}_all_bader_mean_importances.pdf'.format(pair[0],norm_str),format='pdf',dpi=300,transparent=True,bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Store the last model trained for later \n",
    "        # use in comparing against polynomial models.\n",
    "        models_by_pair[str(pair)+'-Bader'] = forest_b\n",
    "        models_by_pair_nn[str(pair)+'-Bader'] = nn_b\n",
    "\n",
    "        bader_perf_by_pair[pair[0]+'-guesses'] = forest_b.predict(xb_valid)\n",
    "        bader_perf_by_pair[pair[0]+'-labels'] = yb_valid\n",
    "\n",
    "        bader_perf_by_pair_nn[pair[0]+'-guesses'] = nn_b.predict(xcnn(xb_valid))\n",
    "        bader_perf_by_pair_nn[pair[0]+'-labels'] = yb_valid\n",
    "\n",
    "        print(\"Done with Bader for \",pair)\n",
    "\n",
    "    ##############################\n",
    "    #   MD PART\n",
    "    ##############################\n",
    "\n",
    "    xmd_train = np.array(ttmd_by_pair[pair]['train_x'])\n",
    "    ymd_train = np.array(ttmd_by_pair[pair]['train_y'])\n",
    "    xmd_train = x_augmentation(xmd_train, delta=0.5)\n",
    "    ymd_train = y_augmentation(ymd_train, delta=0.03)\n",
    "    \n",
    "    if use_test:\n",
    "        xmd_valid = np.array(ttmd_by_pair[pair]['test_x'])\n",
    "        ymd_valid = np.array(ttmd_by_pair[pair]['test_y'])\n",
    "    else:\n",
    "        xmd_valid = np.array(ttmd_by_pair[pair]['valid_x'])\n",
    "        ymd_valid = np.array(ttmd_by_pair[pair]['valid_y'])\n",
    "        xmd_valid = x_augmentation(xmd_valid, delta=0.5)\n",
    "        ymd_valid = y_augmentation(ymd_valid, delta=0.03)\n",
    "    \n",
    "    md_perf_by_pair[pair[0]+'-labels'] = ymd_valid\n",
    "    \n",
    "    if run_md:\n",
    "        forest_md.fit(xmd_train,ymd_train)    \n",
    "        nn_md.fit(xcnn(xmd_train), ymd_train)\n",
    "\n",
    "        cur_model_importances = [forest_md.feature_importances_] \n",
    "        cur_model_accuracies = [r2_score(ymd_valid,forest_md.predict(xmd_valid))] # R-squared\n",
    "        cur_model_maes = [np.mean(np.abs(forest_md.predict(xmd_valid) - ymd_valid))]\n",
    "\n",
    "        nn_model_accuracies = [r2_score(ymd_valid,nn_md.predict(xcnn(xmd_valid)))] # R-squared\n",
    "        nn_model_maes = [np.mean(np.abs(nn_md.predict(xcnn(xmd_valid)) - ymd_valid))]\n",
    "\n",
    "        for i in tqdm(range(RANK_REPEAT), ncols=90, desc='Cross Validation MD'):\n",
    "            forest_md.random_state = rseed+i+1\n",
    "            forest_md.fit(xmd_train,ymd_train)\n",
    "            cur_model_importances.append(forest_md.feature_importances_)\n",
    "            cur_model_accuracies.append(r2_score(ymd_valid,forest_md.predict(xmd_valid)))\n",
    "            cur_model_maes.append(np.mean(np.abs(forest_md.predict(xmd_valid) - ymd_valid)))\n",
    "\n",
    "            nn_md.fit(xcnn(xmd_train),ymd_train)\n",
    "            nn_model_accuracies.append(r2_score(ymd_valid,nn_md.predict(xcnn(xmd_valid))))\n",
    "            nn_model_maes.append(np.mean(np.abs(nn_md.predict(xcnn(xmd_valid)) - ymd_valid)))\n",
    "\n",
    "        importances_mean = np.mean(cur_model_importances,axis=0)\n",
    "        md_accuracies_mean  = np.mean(cur_model_accuracies,axis=0)\n",
    "        md_maes_mean = np.mean(cur_model_maes,axis=0)\n",
    "\n",
    "        nn_md_accuracies_mean  = np.mean(nn_model_accuracies)\n",
    "        nn_md_maes_mean = np.mean(nn_model_maes)\n",
    "\n",
    "        if RANK_REPEAT:\n",
    "            importances_std  = np.std(cur_model_importances,axis=0)\n",
    "            md_accuracies_std   = np.std(cur_model_accuracies,axis=0)\n",
    "            md_maes_std = np.std(cur_model_maes,axis=0)\n",
    "            nn_md_accuracies_std   = np.std(nn_model_accuracies)\n",
    "            nn_md_maes_std = np.std(nn_model_maes)\n",
    "        else:\n",
    "            importances_std  = np.zeros(len(cur_model_importances),axis=0)\n",
    "            md_accuracies_std   = np.zeros(len(cur_model_accuracies),axis=0)\n",
    "            md_maes_std = np.zeros(len(cur_model_maes),axis=0)\n",
    "            nn_md_accuracies_std = np.zeros(len(nn_model_accuracies))\n",
    "            nn_md_maes_std = np.zeros(len(nn_model_maes))\n",
    "\n",
    "        means_by_pair[str(pair)+'-md'] = importances_mean\n",
    "\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(16,9))\n",
    "        plt.errorbar(x_domains[pair],importances_mean,yerr= importances_std,\n",
    "                     label='RF: $\\mu$ $\\pm$ 1 $\\sigma$ (N={})'.format(1+RANK_REPEAT),\n",
    "                     color='black',ecolor='green')\n",
    "        plt.title(\"Model Importance Spread \\nFor {} Mean Distance (Pointwise)\".format(pair[0]))\n",
    "        plt.legend()\n",
    "        plt.savefig(figure_write_folder+'/{}_{}_all_md_mean_importances.pdf'.format(pair[0],norm_str),format='pdf',dpi=300,transparent=True,bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        models_by_pair[str(pair)+'-Mean'] = forest_md\n",
    "        models_by_pair_nn[str(pair)+'-Mean'] = nn_md\n",
    "\n",
    "        # Used in constructing parity plots later\n",
    "\n",
    "        md_perf_by_pair[pair[0]+'-guesses'] = forest_md.predict(xmd_valid)    \n",
    "        md_perf_by_pair_nn[pair[0]+'-guesses'] = nn_md.predict(xcnn(xmd_valid))    \n",
    "    \n",
    "        print(\"Done with mean distance\")\n",
    "        \n",
    "    if not (run_bader and run_md and run_coord):\n",
    "        continue\n",
    "    \n",
    "       \n",
    "    if RANK_REPEAT:\n",
    "        #AVERAGES\n",
    "        accuracies[str(pair)+'-Coord'] = np.round(coord_accuracies_mean*100,4)\n",
    "        accuracies[str(pair)+'-Coord-F1'] = np.round(coord_f1s_mean,2)\n",
    "        accuracies[str(pair)+'-GuessMode'] = np.round(mode_guess_score*100,2)\n",
    "        \n",
    "        accuracies[str(pair)+'-Bader'] = np.round(bader_accuracies_mean*100,4)\n",
    "        accuracies[str(pair)+'-Bader-MAE'] = np.round(bader_maes_mean,2)\n",
    "        \n",
    "        accuracies[str(pair)+'-MeanDist'] = np.round(md_accuracies_mean*100,4)\n",
    "        accuracies[str(pair)+'-MeanDist-MAE'] = np.round(md_maes_mean,3)\n",
    "\n",
    "\n",
    "        accuracies_nn[str(pair)+'-Coord'] = np.round(nn_coord_accuracies_mean*100,4)\n",
    "        accuracies_nn[str(pair)+'-Coord-F1'] = np.round(nn_coord_f1s_mean,2)\n",
    "        accuracies_nn[str(pair)+'-GuessMode'] = np.round(mode_guess_score*100,2)\n",
    "        \n",
    "        accuracies_nn[str(pair)+'-Bader'] = np.round(nn_bader_accuracies_mean*100,4)\n",
    "        accuracies_nn[str(pair)+'-Bader-MAE'] = np.round(nn_bader_maes_mean,2)\n",
    "        \n",
    "        accuracies_nn[str(pair)+'-MeanDist'] = np.round(nn_md_accuracies_mean*100,4)\n",
    "        accuracies_nn[str(pair)+'-MeanDist-MAE'] = np.round(nn_md_maes_mean,3)\n",
    "        \n",
    "        # DEVIATIONS\n",
    "        deviations[str(pair)+'-Coord'] = np.round(coord_accuracies_std*100,4)\n",
    "        deviations[str(pair)+'-Coord-F1'] = np.round(coord_f1s_std,4)\n",
    "        \n",
    "        deviations[str(pair)+'-Bader'] = np.round(bader_accuracies_std*100,4)\n",
    "        deviations[str(pair)+'-Bader-MAE'] = np.round(bader_maes_std,4)\n",
    "        \n",
    "        deviations[str(pair)+'-MeanDist'] = np.round(md_accuracies_std*100,4)\n",
    "        deviations[str(pair)+'-MeanDist-MAE'] = np.round(md_maes_std,4)                    \n",
    "\n",
    "        deviations_nn[str(pair)+'-Coord'] = np.round(nn_coord_accuracies_std*100,4)\n",
    "        deviations_nn[str(pair)+'-Coord-F1'] = np.round(nn_coord_f1s_std,4)\n",
    "        \n",
    "        deviations_nn[str(pair)+'-Bader'] = np.round(nn_bader_accuracies_std*100,4)\n",
    "        deviations_nn[str(pair)+'-Bader-MAE'] = np.round(nn_bader_maes_std,4)\n",
    "        \n",
    "        deviations_nn[str(pair)+'-MeanDist'] = np.round(nn_md_accuracies_std*100,4)\n",
    "        deviations_nn[str(pair)+'-MeanDist-MAE'] = np.round(nn_md_maes_std,4)                    \n",
    "    else:\n",
    "        accuracies[str(pair)+'-Coord'] = np.round(forest_c.score(xc_valid,yc_valid),2)\n",
    "        accuracies[str(pair)+'-Coord-F1'] = np.round(avg_f1_score(guesses=guesses,labels=yc_valid),2)\n",
    "        accuracies[str(pair)+'-GuessMode'] = np.round(mode_guess_score,2)\n",
    "        \n",
    "        accuracies[str(pair)+'-Bader'] = np.round(forest_b.score(xb_valid,yb_valid),2)\n",
    "        accuracies[str(pair)+'-Bader-MAE'] = np.round(np.abs(forest_b.predict(xb_valid)-yb_valid).mean(),2)\n",
    "        \n",
    "        accuracies[str(pair)+'-MeanDist'] = np.round(forest_md.score(xmd_valid,ymd_valid),2)\n",
    "        accuracies[str(pair)+'-MeanDist-MAE'] = np.round(np.abs(forest_md.predict(xmd_valid)-ymd_valid).mean(),2)\n",
    "                         \n",
    "        deviations[str(pair)+'-Coord'] = 0\n",
    "        deviations[str(pair)+'-Coord-F1'] = 0\n",
    "        deviations[str(pair)+'-GuessMode'] =0\n",
    "        \n",
    "        deviations[str(pair)+'-Bader'] = 0\n",
    "        deviations[str(pair)+'-Bader-MAE'] = 0\n",
    "        \n",
    "        deviations[str(pair)+'-MeanDist'] =0\n",
    "        deviations[str(pair)+'-MeanDist-MAE'] = 0\n",
    "                                  \n",
    "    \n",
    "    all_data_values.append([pair[0],\n",
    "                            accuracies[str(pair)+'-Coord'],\n",
    "                            deviations[str(pair)+'-Coord'],\n",
    "                            accuracies[str(pair)+'-Coord-F1'],\n",
    "                            deviations[str(pair)+'-Coord-F1'],\n",
    "                            accuracies[str(pair)+'-GuessMode'],\n",
    "                            accuracies[str(pair)+'-Bader'],\n",
    "                            deviations[str(pair)+'-Bader'],\n",
    "                            accuracies[str(pair)+'-Bader-MAE'],\n",
    "                            deviations[str(pair)+'-Bader-MAE'],\n",
    "                            accuracies[str(pair)+'-MeanDist'],\n",
    "                            deviations[str(pair)+'-MeanDist'],\n",
    "                            accuracies[str(pair)+'-MeanDist-MAE'],\n",
    "                            deviations[str(pair)+'-MeanDist-MAE']]\n",
    "                          )\n",
    "    all_data_values_nn.append([pair[0],\n",
    "                            accuracies_nn[str(pair)+'-Coord'],\n",
    "                            deviations_nn[str(pair)+'-Coord'],\n",
    "                            accuracies_nn[str(pair)+'-Coord-F1'],\n",
    "                            deviations_nn[str(pair)+'-Coord-F1'],\n",
    "                            accuracies_nn[str(pair)+'-GuessMode'],\n",
    "                            accuracies_nn[str(pair)+'-Bader'],\n",
    "                            deviations_nn[str(pair)+'-Bader'],\n",
    "                            accuracies_nn[str(pair)+'-Bader-MAE'],\n",
    "                            deviations_nn[str(pair)+'-Bader-MAE'],\n",
    "                            accuracies_nn[str(pair)+'-MeanDist'],\n",
    "                            deviations_nn[str(pair)+'-MeanDist'],\n",
    "                            accuracies_nn[str(pair)+'-MeanDist-MAE'],\n",
    "                            deviations_nn[str(pair)+'-MeanDist-MAE']]\n",
    "                          )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Performance Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=['Material', \n",
    "         'Coord Baseline', \n",
    "         'Coord Acc.',\n",
    "         'Coord F1 (4)','Coord F1 (5)','Coord F1 (6)',\n",
    "         'Bader $R^2$', \n",
    "         'Bader MAE', \n",
    "         'Mean NN $R^2$',\n",
    "         'Mean NN-MAE',]\n",
    "\n",
    "### Random Forestmodel\n",
    "f = open(figure_write_folder+'/pointwise_table_{}.csv'.format(norm_str),'w')\n",
    "print(str(headers).strip('[').strip(']').replace(\"'\",\"\"))\n",
    "f.write(str(headers).strip('[').strip(']').replace(\"'\",\"\")+'\\n')\n",
    "avgs = [0 for _ in range(len(headers))]\n",
    "\n",
    "for pair in target_elements_groups:\n",
    "    i=1\n",
    "    \n",
    "    elt = pair[0]\n",
    "    \n",
    "    the_str = elt+','\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies[str(pair)+'-GuessMode'] +','\n",
    "    avgs[i] += accuracies[str(pair)+'-GuessMode']; i+=1;\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies[str(pair)+'-Coord'] +' $\\pm$ '\n",
    "    the_str += \"%.2f\" %deviations[str(pair)+'-Coord']  +','\n",
    "    avgs[i] += accuracies[str(pair)+'-Coord']; i+=1;\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies[str(pair)+'-Coord-F1'][0] + ' $ \\pm$ '\n",
    "    the_str += \"%.2f\" %deviations[str(pair)+'-Coord-F1'][0] + ', '\n",
    "    avgs[i] += accuracies[str(pair)+'-Coord-F1'][0]; i+=1;\n",
    "\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies[str(pair)+'-Coord-F1'][1] + ' $ \\pm$ '\n",
    "    the_str += \"%.2f\" %deviations[str(pair)+'-Coord-F1'][1] + ', '\n",
    "    avgs[i] += accuracies[str(pair)+'-Coord-F1'][1]; i+=1;\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies[str(pair)+'-Coord-F1'][2] + ' $ \\pm$ '\n",
    "    the_str += \"%.2f\" %deviations[str(pair)+'-Coord-F1'][2] + ', '\n",
    "    avgs[i] += accuracies[str(pair)+'-Coord-F1'][2]; i+=1;\n",
    "\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies[str(pair)+'-Bader'] +' $\\pm$'\n",
    "    the_str += \"%.2f\" %deviations[str(pair)+'-Bader'] +','\n",
    "    avgs[i] += accuracies[str(pair)+'-Bader']; i+=1;\n",
    "\n",
    "    \n",
    "    the_str += \"%.3f\" %accuracies[str(pair)+'-Bader-MAE'] +' $\\pm$'\n",
    "    the_str += \"%.3f\" %deviations[str(pair)+'-Bader-MAE'] +' , '\n",
    "    avgs[i] += accuracies[str(pair)+'-Bader-MAE']; i+=1;\n",
    "\n",
    "\n",
    "    the_str += \"%.2f\" %accuracies[str(pair)+'-MeanDist'] +' $\\pm$'\n",
    "    the_str += \"%.2f\" %deviations[str(pair)+'-MeanDist'] +','\n",
    "    avgs[i] += accuracies[str(pair)+'-MeanDist']; i+=1;\n",
    "\n",
    "    \n",
    "    the_str += \"%.3f\" %accuracies[str(pair)+'-MeanDist-MAE'] +' $\\pm$'\n",
    "    the_str += \"%.3f\" %deviations[str(pair)+'-MeanDist-MAE']\n",
    "    avgs[i] += accuracies[str(pair)+'-MeanDist-MAE']; i+=1;\n",
    "\n",
    "    f.write(the_str+'\\n')\n",
    "\n",
    "avgs = list(np.round(np.array(avgs)/8,2))\n",
    "avgs[0]='Avgs.'\n",
    "f.write(str(avgs).strip('[').strip(']'))\n",
    "f.close()\n",
    "\n",
    "### Neural Networks model\n",
    "f = open(figure_write_folder+'/pointwise_table_{}_nn.csv'.format(norm_str),'w')\n",
    "print(str(headers).strip('[').strip(']').replace(\"'\",\"\"))\n",
    "f.write(str(headers).strip('[').strip(']').replace(\"'\",\"\")+'\\n')\n",
    "avgs = [0 for _ in range(len(headers))]\n",
    "\n",
    "for pair in target_elements_groups:\n",
    "    i=1\n",
    "    \n",
    "    elt = pair[0]\n",
    "    \n",
    "    the_str = elt+','\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies_nn[str(pair)+'-GuessMode'] +','\n",
    "    avgs[i] += accuracies_nn[str(pair)+'-GuessMode']; i+=1;\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies_nn[str(pair)+'-Coord'] +' $\\pm$ '\n",
    "    the_str += \"%.2f\" %deviations_nn[str(pair)+'-Coord']  +','\n",
    "    avgs[i] += accuracies_nn[str(pair)+'-Coord']; i+=1;\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies_nn[str(pair)+'-Coord-F1'][0] + ' $ \\pm$ '\n",
    "    the_str += \"%.2f\" %deviations_nn[str(pair)+'-Coord-F1'][0] + ', '\n",
    "    avgs[i] += accuracies_nn[str(pair)+'-Coord-F1'][0]; i+=1;\n",
    "\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies_nn[str(pair)+'-Coord-F1'][1] + ' $ \\pm$ '\n",
    "    the_str += \"%.2f\" %deviations_nn[str(pair)+'-Coord-F1'][1] + ', '\n",
    "    avgs[i] += accuracies_nn[str(pair)+'-Coord-F1'][1]; i+=1;\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies_nn[str(pair)+'-Coord-F1'][2] + ' $ \\pm$ '\n",
    "    the_str += \"%.2f\" %deviations_nn[str(pair)+'-Coord-F1'][2] + ', '\n",
    "    avgs[i] += accuracies_nn[str(pair)+'-Coord-F1'][2]; i+=1;\n",
    "\n",
    "    \n",
    "    the_str += \"%.2f\" %accuracies_nn[str(pair)+'-Bader'] +' $\\pm$'\n",
    "    the_str += \"%.2f\" %deviations_nn[str(pair)+'-Bader'] +','\n",
    "    avgs[i] += accuracies_nn[str(pair)+'-Bader']; i+=1;\n",
    "\n",
    "    \n",
    "    the_str += \"%.3f\" %accuracies_nn[str(pair)+'-Bader-MAE'] +' $\\pm$'\n",
    "    the_str += \"%.3f\" %deviations_nn[str(pair)+'-Bader-MAE'] +' , '\n",
    "    avgs[i] += accuracies_nn[str(pair)+'-Bader-MAE']; i+=1;\n",
    "\n",
    "\n",
    "    the_str += \"%.2f\" %accuracies_nn[str(pair)+'-MeanDist'] +' $\\pm$'\n",
    "    the_str += \"%.2f\" %deviations_nn[str(pair)+'-MeanDist'] +','\n",
    "    avgs[i] += accuracies_nn[str(pair)+'-MeanDist']; i+=1;\n",
    "\n",
    "    \n",
    "    the_str += \"%.3f\" %accuracies_nn[str(pair)+'-MeanDist-MAE'] +' $\\pm$'\n",
    "    the_str += \"%.3f\" %deviations_nn[str(pair)+'-MeanDist-MAE']\n",
    "    avgs[i] += accuracies_nn[str(pair)+'-MeanDist-MAE']; i+=1;\n",
    "\n",
    "    f.write(the_str+'\\n')\n",
    "\n",
    "avgs = list(np.round(np.array(avgs)/8,2))\n",
    "avgs[0]='Avgs.'\n",
    "f.write(str(avgs).strip('[').strip(']'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = []\n",
    "for pair in tqdm(target_elements_groups, ncols=90, desc='target_elements_groups'):\n",
    "    #############################################\n",
    "    # COORDINATION\n",
    "    #############################################\n",
    "    yc_train = ttc_by_pair[pair]['train_y'] \n",
    "    yc_train = y_augmentation(yc_train, delta=0.00)\n",
    "\n",
    "    #############################################\n",
    "    # BADER\n",
    "    #############################################\n",
    "    yb_train =  ttb_by_pair[pair]['train_y']\n",
    "    yb_train = y_augmentation(yb_train, delta=0.02)\n",
    "    \n",
    "    ##############################\n",
    "    #   MD PART\n",
    "    ##############################\n",
    "    ymd_train = np.array(ttmd_by_pair[pair]['train_y'])\n",
    "    ymd_train = y_augmentation(ymd_train, delta=0.02)\n",
    "    \n",
    "    data_size.append([len(yc_train), len(yb_train), len(ymd_train)])\n",
    "\n",
    "data_size_df = pandas.DataFrame(data_size)\n",
    "data_size_df.columns = ['Size Coord', 'Size Bader', 'Size Mean NN']\n",
    "data_size_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Table: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = pandas.read_csv(figure_write_folder+'/pointwise_table_{}.csv'.format(norm_str))\n",
    "pandas.concat([rf_df, data_size_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Table: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_df = pandas.read_csv(figure_write_folder+'/pointwise_table_{}_nn.csv'.format(norm_str))\n",
    "pandas.concat([cnn_df, data_size_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Accumulated Figures of Merit\n",
    "## for Pointwise  models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(dpi=300)\n",
    "for pj, pair in enumerate(target_elements_groups):\n",
    "    \n",
    "    \n",
    "    plt.bar(pj+.5,accuracies[str(pair)+'-Coord']/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           label=str(pair[0]),\n",
    "           yerr = deviations[str(pair)+'-Coord']/100,capsize=1.5)\n",
    "    plt.bar(pj+.5,accuracies[str(pair)+'-GuessMode']/100,color='lightgrey',width=1,alpha=1)\n",
    "    plt.bar(pj+.5+8,accuracies[str(pair)+'-Coord-F1'][0]/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr = deviations[str(pair)+'-Coord-F1'][0]/100, capsize=1.5)\n",
    "    \n",
    "    plt.bar(pj+.5+2*8,accuracies[str(pair)+'-Coord-F1'][1]/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr = deviations[str(pair)+'-Coord-F1'][1]/100,capsize=1.5)\n",
    "    \n",
    "    plt.bar(pj+.5+3*8,accuracies[str(pair)+'-Coord-F1'][2]/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr = deviations[str(pair)+'-Coord-F1'][2]/100,capsize=1.5)\n",
    "    \n",
    "    plt.bar(pj+.5+4*8,accuracies[str(pair)+'-MeanDist']/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr =  deviations[str(pair)+'-MeanDist']/100,capsize=1.5)\n",
    "    \n",
    "    plt.bar(pj+.5+5*8,accuracies[str(pair)+'-Bader']/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr =  deviations[str(pair)+'-Bader']/100,capsize=1.5)\n",
    "    \n",
    "    \n",
    "for i in [0,8,16,24,32,40]:\n",
    "    plt.axvline(i,color='black',lw=1,ls='-')\n",
    "\n",
    "for i in [.2,.4,.6,.8]:\n",
    "    plt.axhline(i,color='gray',ls='--',alpha=.1)\n",
    "\n",
    "plt.xticks([8*i+4  for i in range(0,6)],\n",
    "                labels=['Coordination\\n Number (CN)\\n Accuracy', 'F1\\n(CN=4)', 'F1\\n(CN=5)', 'F1\\n(CN=6)',\n",
    "                                      'Mean NN.\\nDist. $R^2$','Bader\\n$R^2$'],)\n",
    "plt.xticks()\n",
    "plt.xlim(0,6*7+6)\n",
    "plt.ylim(-0.05,1.0000001)\n",
    "plt.ylabel('Figure of Merit')\n",
    "\n",
    "\n",
    "plt.title(\"RF: Accumulated Figures of Merit\\n(Pointwise)\")\n",
    "\n",
    "plt.legend(loc='lower center',ncol=2,framealpha=.95)\n",
    "plt.savefig(f'{figure_write_folder}/{norm_str}_all_perf.svg',format='svg',dpi=300,bbox_inches='tight',transprent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(dpi=300)\n",
    "for pj, pair in enumerate(target_elements_groups):\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.bar(pj+.5,accuracies_nn[str(pair)+'-Coord']/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           label=str(pair[0]),\n",
    "           yerr = deviations_nn[str(pair)+'-Coord']/100,capsize=1.5)\n",
    "    plt.bar(pj+.5,accuracies_nn[str(pair)+'-GuessMode']/100,color='lightgrey',width=1,alpha=1)\n",
    "    plt.bar(pj+.5+8,accuracies_nn[str(pair)+'-Coord-F1'][0]/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr = deviations_nn[str(pair)+'-Coord-F1'][0]/100, capsize=1.5)\n",
    "    \n",
    "    plt.bar(pj+.5+2*8,accuracies_nn[str(pair)+'-Coord-F1'][1]/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr = deviations_nn[str(pair)+'-Coord-F1'][1]/100,capsize=1.5)\n",
    "    \n",
    "    plt.bar(pj+.5+3*8,accuracies_nn[str(pair)+'-Coord-F1'][2]/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr = deviations_nn[str(pair)+'-Coord-F1'][2]/100,capsize=1.5)\n",
    "    \n",
    "    plt.bar(pj+.5+4*8,accuracies_nn[str(pair)+'-MeanDist']/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr =  deviations_nn[str(pair)+'-MeanDist']/100,capsize=1.5)\n",
    "    \n",
    "    plt.bar(pj+.5+5*8,accuracies_nn[str(pair)+'-Bader']/100,color=colors_by_pair[pair],width=1,alpha=.7,\n",
    "           yerr =  deviations_nn[str(pair)+'-Bader']/100,capsize=1.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for i in [0,8,16,24,32,40]:\n",
    "    plt.axvline(i,color='black',lw=1,ls='-')\n",
    "\n",
    "for i in [.2,.4,.6,.8]:\n",
    "    plt.axhline(i,color='gray',ls='--',alpha=.1)\n",
    "\n",
    "plt.xticks([8*i+4  for i in range(0,6)],\n",
    "                labels=['Coordination\\n Number (CN)\\n Accuracy', 'F1\\n(CN=4)', 'F1\\n(CN=5)', 'F1\\n(CN=6)',\n",
    "                                      'Mean NN.\\nDist. $R^2$','Bader\\n$R^2$'],)\n",
    "plt.xticks()\n",
    "plt.xlim(-0,6*7+6)\n",
    "plt.ylim(-0.05,1.0000001)\n",
    "plt.ylabel('Figure of Merit')\n",
    "\n",
    "\n",
    "plt.title(\"CNN: Accumulated Figures of Merit\\n(Pointwise)\")\n",
    "\n",
    "plt.legend(loc='lower center',ncol=2,framealpha=.95)\n",
    "plt.savefig(f'{figure_write_folder}/{norm_str}_all_perf_nn.svg',format='svg',dpi=300,bbox_inches='tight',transprent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of MD Fitting- Uniparity\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pair_to_icon={'Ti':\"o\",\n",
    "              'V':'v',\n",
    "              'Cr':'^',\n",
    "              'Mn':\"s\",\n",
    "              'Fe':\"P\",\n",
    "              'Co':\"h\",\n",
    "             'Ni':'D',\n",
    "              'Cu':'p'}\n",
    "all_min = 100\n",
    "all_max = 0\n",
    "plt.clf()\n",
    "plt.figure(figsize=(4,4),dpi=300)\n",
    "for pair in target_elements_groups:\n",
    "    number = len(md_perf_by_pair[pair[0]+'-guesses'])\n",
    "    all_min = min(all_min, min(md_perf_by_pair[pair[0]+'-labels']),min(md_perf_by_pair[pair[0]+'-guesses']))\n",
    "    all_max = max(all_max, max(md_perf_by_pair[pair[0]+'-labels']),max(md_perf_by_pair[pair[0]+'-guesses']))\n",
    "\n",
    "    plt.scatter(md_perf_by_pair[pair[0]+'-labels'][0],\n",
    "                    md_perf_by_pair[pair[0]+'-guesses'][0],\n",
    "                    zorder=-1,\n",
    "                    marker=pair_to_icon[pair[0]],\n",
    "                    color= colors_by_pair[pair], \n",
    "                    alpha = 1,\n",
    "                    label = pair[0] + \" MAE: \" +str(np.round((accuracies[str(pair)+'-MeanDist-MAE']),3)))\n",
    "    #print(number, len(md_perf_by_pair[pair[0]+'-labels']))\n",
    "    for i in range(number)[1:]:\n",
    "        plt.scatter(md_perf_by_pair[pair[0]+'-labels'][i],\n",
    "                    md_perf_by_pair[pair[0]+'-guesses'][i],\n",
    "                    zorder=np.random.uniform(0,1),\n",
    "                    marker=pair_to_icon[pair[0]],\n",
    "                    color= colors_by_pair[pair], \n",
    "                    alpha = 100/number)\n",
    "\n",
    "plt.plot((all_min,all_max),(all_min,all_max),color='black',ls='--')\n",
    "plt.legend(fontsize=8)\n",
    "plt.title(\"RF: Mean Nearest-Neighbor Distance\\nRegression Performance\")\n",
    "plt.xlabel(\"True Distance ($\\AA$)\")\n",
    "plt.ylabel(\"Predicted Distance ($\\AA$)\")\n",
    "\n",
    "plt.savefig(f'{figure_write_folder}/{norm_str}_md_uniparity.svg',format='svg',dpi=300,bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_to_icon={'Ti':\"o\",\n",
    "              'V':'v',\n",
    "              'Cr':'^',\n",
    "              'Mn':\"s\",\n",
    "              'Fe':\"P\",\n",
    "              'Co':\"h\",\n",
    "             'Ni':'D',\n",
    "              'Cu':'p'}\n",
    "all_min = 100\n",
    "all_max = 0\n",
    "plt.clf()\n",
    "plt.figure(figsize=(4,4),dpi=300)\n",
    "for pair in target_elements_groups:\n",
    "    number = len(md_perf_by_pair_nn[pair[0]+'-guesses'])\n",
    "    all_min = min(all_min, min(md_perf_by_pair[pair[0]+'-labels']),min(md_perf_by_pair_nn[pair[0]+'-guesses']))\n",
    "    all_max = max(all_max, max(md_perf_by_pair[pair[0]+'-labels']),max(md_perf_by_pair_nn[pair[0]+'-guesses']))\n",
    "\n",
    "    plt.scatter(md_perf_by_pair[pair[0]+'-labels'][0],\n",
    "                    md_perf_by_pair_nn[pair[0]+'-guesses'][0],\n",
    "                    zorder=-1,\n",
    "                    marker=pair_to_icon[pair[0]],\n",
    "                    color= colors_by_pair[pair], \n",
    "                    alpha = 1,\n",
    "                    label = pair[0] + \" MAE: \" +str(np.round((accuracies_nn[str(pair)+'-MeanDist-MAE']),3)))\n",
    "    #print(number, len(md_perf_by_pair[pair[0]+'-labels']))\n",
    "    for i in range(number)[1:]:\n",
    "        plt.scatter(md_perf_by_pair[pair[0]+'-labels'][i],\n",
    "                    md_perf_by_pair_nn[pair[0]+'-guesses'][i],\n",
    "                    zorder=np.random.uniform(0,1),\n",
    "                    marker=pair_to_icon[pair[0]],\n",
    "                    color= colors_by_pair[pair], \n",
    "                    alpha = 100/number)\n",
    "\n",
    "plt.plot((all_min,all_max),(all_min,all_max),color='black',ls='--')\n",
    "plt.legend(fontsize=8)\n",
    "plt.title(\"CNN: Mean Nearest-Neighbor Distance\\nRegression Performance\")\n",
    "plt.xlabel(\"True Distance ($\\AA$)\")\n",
    "plt.ylabel(\"Predicted Distance ($\\AA$)\")\n",
    "\n",
    "plt.savefig(f'{figure_write_folder}/{norm_str}_md_uniparity_nn.svg',format='svg',dpi=300,bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bader Performance\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pair_to_icon={'Ti':\"o\",\n",
    "              'V':'v',\n",
    "              'Cr':'^',\n",
    "              'Mn':\"s\",\n",
    "              'Fe':\"P\",\n",
    "              'Co':\"h\",\n",
    "             'Ni':'D',\n",
    "              'Cu':'p'}\n",
    "all_min = 100\n",
    "all_max = 0\n",
    "plt.clf()\n",
    "plt.figure(figsize=(4,4),dpi=300)\n",
    "for pair in target_elements_groups:\n",
    "    number = len(bader_perf_by_pair[pair[0]+'-guesses'])\n",
    "    all_min = min(all_min, min(bader_perf_by_pair[pair[0]+'-labels']),min(bader_perf_by_pair[pair[0]+'-guesses']))\n",
    "    all_max = max(all_max, max(bader_perf_by_pair[pair[0]+'-labels']),max(bader_perf_by_pair[pair[0]+'-guesses']))\n",
    "\n",
    "    plt.scatter(bader_perf_by_pair[pair[0]+'-labels'][0],\n",
    "                bader_perf_by_pair[pair[0]+'-guesses'][0],\n",
    "                zorder=-1,\n",
    "                marker=pair_to_icon[pair[0]],\n",
    "                color= colors_by_pair[pair], \n",
    "                alpha = 1,\n",
    "                label=pair[0]+\" MAE: \"\n",
    "                +str(np.round(accuracies[str(pair)+'-Bader-MAE'],2))) \n",
    "                #+f\"\\t R$^2$:{accuracies[str(pair)+'-Bader']:.2f}\") \n",
    "    #print(number, len(md_perf_by_pair[pair[0]+'-labels']))\n",
    "    for i in range(number)[1:]:\n",
    "        plt.scatter(bader_perf_by_pair[pair[0]+'-labels'][i],\n",
    "                    bader_perf_by_pair[pair[0]+'-guesses'][i],\n",
    "                    zorder=np.random.uniform(0,1),\n",
    "                    marker=pair_to_icon[pair[0]],\n",
    "                    color= colors_by_pair[pair], \n",
    "                    alpha = 100/number)\n",
    "\n",
    "plt.plot((all_min,all_max),(all_min,all_max),color='black',ls='--')\n",
    "plt.legend(fontsize=8,loc='best')\n",
    "plt.title(\"RF: Bader Charge\\nRegression Performance\")\n",
    "plt.xlabel(\"True Charge (e=1)\")\n",
    "plt.ylabel(\"Predicted\\nCharge (e=1)\")\n",
    "plt.savefig(f'{figure_write_folder}/{norm_str}_bader_uniparity.svg',format='svg',dpi=300,bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_to_icon={'Ti':\"o\",\n",
    "              'V':'v',\n",
    "              'Cr':'^',\n",
    "              'Mn':\"s\",\n",
    "              'Fe':\"P\",\n",
    "              'Co':\"h\",\n",
    "             'Ni':'D',\n",
    "              'Cu':'p'}\n",
    "all_min = 100\n",
    "all_max = 0\n",
    "plt.clf()\n",
    "plt.figure(figsize=(4,4),dpi=300)\n",
    "for pair in target_elements_groups:\n",
    "    number = len(bader_perf_by_pair_nn[pair[0]+'-guesses'])\n",
    "    all_min = min(all_min, min(bader_perf_by_pair_nn[pair[0]+'-labels']),min(bader_perf_by_pair_nn[pair[0]+'-guesses']))\n",
    "    all_max = max(all_max, max(bader_perf_by_pair_nn[pair[0]+'-labels']),max(bader_perf_by_pair_nn[pair[0]+'-guesses']))\n",
    "\n",
    "    plt.scatter(bader_perf_by_pair_nn[pair[0]+'-labels'][0],\n",
    "                bader_perf_by_pair_nn[pair[0]+'-guesses'][0],\n",
    "                zorder=-1,\n",
    "                marker=pair_to_icon[pair[0]],\n",
    "                color= colors_by_pair[pair], \n",
    "                alpha = 1,\n",
    "                label=pair[0]+\" MAE: \"\n",
    "                +str(np.round(accuracies_nn[str(pair)+'-Bader-MAE'],2))) \n",
    "                #+f\"\\t R$^2$:{accuracies[str(pair)+'-Bader']:.2f}\") \n",
    "    #print(number, len(md_perf_by_pair[pair[0]+'-labels']))\n",
    "    for i in range(number)[1:]:\n",
    "        plt.scatter(bader_perf_by_pair_nn[pair[0]+'-labels'][i],\n",
    "                    bader_perf_by_pair_nn[pair[0]+'-guesses'][i],\n",
    "                    zorder=np.random.uniform(0,1),\n",
    "                    marker=pair_to_icon[pair[0]],\n",
    "                    color= colors_by_pair[pair], \n",
    "                    alpha = 100/number)\n",
    "\n",
    "plt.plot((all_min,all_max),(all_min,all_max),color='black',ls='--')\n",
    "plt.legend(fontsize=8,loc='best')\n",
    "plt.title(\"CNN: Bader Charge\\nRegression Performance\")\n",
    "plt.xlabel(\"True Charge (e=1)\")\n",
    "plt.ylabel(\"Predicted\\nCharge (e=1)\")\n",
    "plt.savefig(f'{figure_write_folder}/{norm_str}_bader_uniparity_nn.svg',format='svg',dpi=300,bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------\n",
    "\n",
    "#                     PART TWO: POLYNOMIALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Polynomial Fit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Big Cell - Polynomialized Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Performance Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "# Part 3: Plotting Comparative Performance of Training & Testing, and Feature Importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature Rank Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Change in Performance from Pointwise to Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
